# -*- coding: utf-8 -*-
"""Copy of Muhammad Asim Iqbal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sg1SLUdZyjL8423o_L6HJI5rPC-yYUwN
"""

# prompt: data analize by using py spark and install library of pyspark

!pip install pyspark

# prompt: write a code to apply pyspark library function on data set
# data set we provide you
# apply all operation on this data set that are available in the pyspark library

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('pyspark_example').getOrCreate()

# Load the data set
df = spark.read.csv('/content/Attendance data.csv', header=True)

# Print the schema of the data set
df.printSchema()

# Show the first few rows of the data set
df.show(5)

# Select specific columns
df.select('School DBN', 'Date').show()

# Filter the data set
df.filter(df['Absent'] > 10).show()

# Group the data set
df.groupBy('Present').count().show()

# Join two data sets
df1 = df.select('Absent', 'Present')
df2 = df.select('School DBN', 'Enrolled')


# Perform aggregations
df.agg({'Present': 'max', 'Absent': 'min'}).show()

# Use SQL statements
df.createOrReplaceTempView('table_name')
spark.sql('SELECT * FROM table_name WHERE Absent < 10').show()

# prompt: write a code to apply more operation on this data set using pyspark

# Create a new column that calculates the percentage of attendance for each row
df = df.withColumn('Attendance_Percentage', (df['Present'] / df['Enrolled']) * 100)

# Show the first few rows of the new data set
df.show(5)

# Group the data set by school DBN and calculate the average attendance percentage for each school
df.groupBy('School DBN').agg(avg('Attendance_Percentage').alias('Average Attendance Percentage')).show()

# Filter the data set to only include rows where the attendance percentage is greater than 90%
df.filter(df['Attendance_Percentage'] > 90).show()

# Join the data set with another data set that contains information about the schools
school_info_df = spark.read.csv('/content/Attendance data.csv', header=True)
df_joined = df.join(school_info_df, on='School DBN', how='left')

# Show the first few rows of the joined data set
df_joined.show(5)

# prompt: write a code
# which student have absent less then 60 percent
# to show this in  columns formate

df_less_than_60_percent = df.filter(df['Absent'] < (df['Enrolled'] * 0.6))
df_less_than_60_percent.select('School DBN', 'Absent', 'Enrolled').show()

# prompt: write a code to apply more operation using this datas set
# and making graph for big data analytics

# Create a histogram of the attendance percentage


# Create a bar chart of the average attendance percentage for each school
df.groupBy('School DBN').agg(avg('Attendance_Percentage').alias('Average Attendance Percentage')).sort('Average Attendance Percentage').show(truncate=False)

# Create a line chart of the trend in attendance percentage over time
df.groupBy('Date').agg(avg('Attendance_Percentage').alias('Average Attendance Percentage')).sort('Date').show()

# Create a scatter plot of the relationship between attendance percentage and enrollment
df.select('Attendance_Percentage', 'Enrolled').show()

# Create a correlation matrix of all the numerical columns in the data set

# prompt: write to apply statistics and removing null values

# Calculate the mean of the 'Present' column
df.select(mean('Present')).show()

# Calculate the standard deviation of the 'Absent' column
df.select(stddev('Absent')).show()

# Calculate the variance of the 'Present' column
df.select(variance('Present')).show()

# Calculate the covariance between 'Present' and 'Absent' columns


# Calculate the correlation between 'Present' and 'Absent' columns
df.select(corr('Present', 'Absent')).show()

# Drop rows with null values
df.dropna().show()

# Drop columns with null values
df.drop('School DBN').show()

# Fill null values with a specific value
df.na.fill(0).show()

# Fill null values with the mean of the column
df.na.fill(df.select(mean('Present')).collect()[0][0]).show()